# Copyright 2023 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Functions and classes related to data."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any

import torch
from datasets import load_dataset
from torch.utils.data import Dataset, random_split
from tqdm import tqdm
from transformers import PreTrainedTokenizer
from transformers.tokenization_utils import PaddingStrategy, TruncationStrategy

from .constants import PROMPT_INPUT


__all__ = ['load_qa_moderation_dataset']


@dataclass
class TextClassificationData:
    """Data for QA classification."""

    texts: list[str]
    labels: list[int] | list[list[bool]]
    id2label: dict[int, str]


def load_qa_moderation_dataset(
    data_name_or_path: str,
    prompt_input: str = PROMPT_INPUT,
) -> TextClassificationData:
    """Load Hugging Face dataset from a given path or a given name, and preprocess it."""
    name_or_path, _, split = data_name_or_path.partition(':')

    dataset = load_dataset(
        name_or_path,
        split=split or 'train',
    ).shuffle()

    prompts = dataset['prompt']
    responses = dataset['response']
    texts = [prompt_input.format(input=x) + y for x, y in zip(prompts, responses)]

    labels = dataset['category']
    label_names = list(labels[0].keys())
    labels = [list(label.values()) for label in labels]
    id2label = dict(enumerate(label_names))

    return TextClassificationData(texts=texts, labels=labels, id2label=id2label)


def split_dataset(dataset: Dataset, eval_split_ratio: float = 0.1) -> Dataset:
    """Load dataset and split into train and eval."""
    if not 0.0 <= eval_split_ratio <= 1.0:
        raise ValueError('`eval_split_ratio` must be in [0.0, 1.0].')

    if eval_split_ratio == 0.0:
        return dataset, None

    eval_size = round(len(dataset) * eval_split_ratio)
    train_size = len(dataset) - eval_size
    return random_split(dataset, [train_size, eval_size])


class SequenceClassificationDataset(Dataset):
    """Dataset for classification."""

    _SENTINEL: Any = object()

    def __init__(
        self,
        texts: list[str],
        labels: list[int] | list[list[bool]],
        tokenizer: PreTrainedTokenizer,
        lazy_tokenization: bool = False,
    ) -> None:
        super().__init__()

        self.rank = -1
        if torch.distributed.is_initialized():
            self.rank = torch.distributed.get_rank()

        self.labels = torch.tensor(labels, dtype=torch.float32)
        self.raw_texts = [
            text + tokenizer.eos_token if not text.endswith(tokenizer.eos_token) else text
            for text in texts
        ]
        self.tokenizer = tokenizer
        if lazy_tokenization:
            self.input_ids = [self._SENTINEL for _ in range(len(self.raw_texts))]
        else:
            self.input_ids = [
                self.tokenize(text) for text in tqdm(self.raw_texts, disable=self.rank > 0)
            ]

    def __len__(self) -> int:
        return len(self.input_ids)

    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:
        input_ids = self.input_ids[index]
        if input_ids is self._SENTINEL:
            input_ids = self.tokenize(self.raw_texts[index])
            self.input_ids[index] = input_ids
        return {
            'input_ids': input_ids,
            'labels': self.labels[index],
        }

    def tokenize(self, text: str) -> torch.Tensor:
        """Tokenize a given text."""
        return self.tokenizer(
            text,
            add_special_tokens=True,
            padding=PaddingStrategy.LONGEST,
            max_length=self.tokenizer.model_max_length,
            truncation=TruncationStrategy.LONGEST_FIRST,
            return_tensors='pt',
        )['input_ids'][0]


class DataCollatorForSequenceClassificationDataset:
    """Collate examples for classification."""

    def __init__(self, tokenizer: PreTrainedTokenizer) -> None:
        self.tokenizer = tokenizer

    def __call__(self, samples: list[dict]) -> dict[str, torch.Tensor]:
        input_ids = torch.nn.utils.rnn.pad_sequence(
            [sample['input_ids'] for sample in samples],
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id,
        )
        labels = torch.stack([sample['labels'] for sample in samples], dim=0)
        return {
            'input_ids': input_ids,
            'labels': labels,
            'attention_mask': input_ids.ne(self.tokenizer.pad_token_id),
        }
