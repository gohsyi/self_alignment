# Train Moderation with Classification Dataset

Traditional methodologies for content moderation in Question-Answering (QA) tasks assess the harmfulness of a QA pair by evaluating the toxicity of individual utterances. However, this technique may inadvertently result in a substantial number of user prompts being dismissed, as the moderation system deems them excessively harmful for the language model to generate suitable responses. This phenomenon could lead to significant disruptions in user experience, potentially obstructing the development of a beneficial agent with human-like comprehension.

We advocate for a novel paradigm in content moderation for QA tasks - referred to as "QA-moderation":

<div align="center">
  <img src="../../images/moderation.png" width="90%"/>
</div>

 In this paradigm, a QA pair is labeled as harmful or harmless based on its risk neutrality extent, that is, the degree to which potential risks in a potentially harmful question can be mitigated by a benign response.

Here is a training example of QA-Moderation using our Classification Dataset.

## Environment

Change directory to `examples/moderation`:

```bash
cd examples/moderation
```

Setup a conda environment using `conda` / `mamba`:

```bash
conda env create --file conda-recipe.yaml  # or `mamba env create --file conda-recipe.yaml`
```

This will automatically set up all dependencies.

## Training a Moderation Model

```bash
torchrun --nproc_per_node=8 \
    train.py \
    --model_name_or_path "huggyllama/llama-7b" \
    --train_data_name_or_path PKU-Alignment/BeaverTails:train \
    --eval_data_name_or_path PKU-Alignment/BeaverTails:test \
    --output_dir output/beavertails-7b \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "steps" \
    --eval_steps 250 \
    --save_strategy "epoch" \
    --save_total_limit 1 \
    --learning_rate 1e-5 \
    --weight_decay 0.1 \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap "LlamaDecoderLayer" \
    --gradient_checkpointing \
    --bf16 True \
    --tf32 True
```

## Evaluation

Eval the training results with an example dataset ([`examples/evaluation/evaluation.json`](../evaluation)):

```bash
python evaluate.py \
    --eval_dataset ../evaluation/evaluation.json \
    --model_path PKU-Alignment/beaver-dam-7b \
    --max_length 512 \
    --output_dir output/evaluation
```

Then you can get the results like:

<div align="center">
  <img src="../../images/flagged-proportion.png" width="90%"/>
</div>

We also provide the trained weights of our QA-Moderation model on Hugging Face: [`PKU-Alignment/beaver-dam-7b`](https://huggingface.co/PKU-Alignment/beaver-dam-7b).
