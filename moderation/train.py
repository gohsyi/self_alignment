# Copyright 2023 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Train a Moderation Model."""

from __future__ import annotations

import os
from dataclasses import dataclass, field

import transformers
from dataset import (
    DataCollatorForSequenceClassificationDataset,
    SequenceClassificationDataset,
    TextClassificationData,
    load_qa_moderation_dataset,
    split_dataset,
)
from torch.utils.data import Dataset

from .constants import PROMPT_INPUT
from .moderation import QAModeration


@dataclass
class ModelArguments:
    """Arguments related to model."""

    model_name_or_path: str | None = field(default='facebook/opt-125m')


@dataclass
class DataArguments:
    """Arguments related to data."""

    train_data_name_or_path: str | None = field(
        default=None,
        metadata={'help': 'Path to the training data.'},
    )
    eval_data_name_or_path: str | None = field(
        default=None,
        metadata={'help': 'Path to the validation data.'},
    )
    eval_split_ratio: float = field(
        default=None,
        metadata={'help': 'Ratio of the validation data.'},
    )


@dataclass
class TrainingArguments(transformers.TrainingArguments):
    """Arguments related to training."""

    cache_dir: str | None = field(default=None)
    optim: str = field(default='adamw_torch')
    model_max_length: int = field(
        default=512,
        metadata={
            'help': 'Maximum sequence length. Sequences will be right padded (and possibly'
            'truncated).',
        },
    )


def make_sequence_classification_data_module(
    tokenizer: transformers.PreTrainedTokenizer,
    train_data: TextClassificationData,
    eval_data: TextClassificationData | None = None,
    eval_split_ratio: float | None = None,
) -> dict[str, Dataset]:
    """Make dataset and collator for supervised fine-tuning."""

    train_dataset = SequenceClassificationDataset(
        train_data.texts,
        train_data.labels,
        tokenizer=tokenizer,
    )

    if eval_data is not None:
        eval_dataset = SequenceClassificationDataset(
            eval_data.texts,
            eval_data.labels,
            tokenizer=tokenizer,
        )
    elif eval_split_ratio is not None:
        if not isinstance(eval_split_ratio, float) or not 0 < eval_split_ratio < 1:
            raise ValueError('`eval_split_ratio` must be a float between 0 and 1.')
        train_dataset, eval_dataset = split_dataset(
            train_dataset,
            eval_split_ratio=eval_split_ratio,
        )
    else:
        eval_dataset = None
    data_collator = DataCollatorForSequenceClassificationDataset(tokenizer)
    return {
        'train_dataset': train_dataset,
        'eval_dataset': eval_dataset,
        'data_collator': data_collator,
    }


def main() -> None:
    """Train the model."""
    parser = transformers.HfArgumentParser(
        (ModelArguments, DataArguments, TrainingArguments),
    )
    # pylint: disable-next=unbalanced-tuple-unpacking
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    os.makedirs(training_args.output_dir, exist_ok=True)

    if data_args.train_data_name_or_path is None:
        raise ValueError('Must specify `train_data_name_or_path`.')

    train_data = load_qa_moderation_dataset(
        data_args.train_data_name_or_path,
        prompt_input=PROMPT_INPUT,
    )
    eval_data = None
    if data_args.eval_data_name_or_path is not None:
        eval_data = load_qa_moderation_dataset(
            data_args.eval_data_name_or_path,
            prompt_input=PROMPT_INPUT,
        )
    moderation = QAModeration.from_pretrained(
        model_args.model_name_or_path,
        id2label=train_data.id2label,
        problem_type='multi_label_classification',
    )
    data_module = make_sequence_classification_data_module(
        tokenizer=moderation.tokenizer,
        train_data=train_data,
        eval_data=eval_data,
        eval_split_ratio=data_args.eval_split_ratio,
    )
    moderation.fit(
        training_args=training_args,
        train_dataset=data_module['train_dataset'],
        eval_dataset=data_module['eval_dataset'],
        data_collator=data_module['data_collator'],
    )


if __name__ == '__main__':
    main()
